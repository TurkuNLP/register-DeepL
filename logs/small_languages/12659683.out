learning rate: 8e-6 treshold: 0.4 batch: 8 epochs: 5
Namespace(batch=8, checkpoint='../multilabel/transfernodev2', epochs=5, full=False, lang='transfer', learning=8e-06, model='xlm-roberta-large', multilingual=False, saved='saved_models/all_multilingual', test_set=['data/test_sets/main_labels_only/jpn_test_modified.tsv'], train_set=['data/downsampled/main_labels_only/all_downsampled.tsv.gz'], treshold=0.4)
Downloading and preparing dataset csv/default to /users/annieske/.cache/huggingface/datasets/csv/default-a8ab88c008a813b5/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...
Dataset csv downloaded and prepared to /users/annieske/.cache/huggingface/datasets/csv/default-a8ab88c008a813b5/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.
{'test': Dataset({
    features: ['text', 'label'],
    num_rows: 100
}),
 'train': Dataset({
    features: ['text', 'label'],
    num_rows: 8784
})}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.2452, 'learning_rate': 6.4e-06, 'epoch': 1.0}
{'eval_loss': 0.19133609533309937, 'eval_f1': 0.7263681592039802, 'eval_roc_auc': 0.8364349291693945, 'eval_accuracy': 0.66, 'eval_runtime': 3.1804, 'eval_samples_per_second': 31.443, 'eval_steps_per_second': 1.258, 'epoch': 1.0}
{'loss': 0.1617, 'learning_rate': 4.8e-06, 'epoch': 2.0}
{'eval_loss': 0.22401496767997742, 'eval_f1': 0.6636363636363636, 'eval_roc_auc': 0.8228050869886198, 'eval_accuracy': 0.59, 'eval_runtime': 3.1673, 'eval_samples_per_second': 31.573, 'eval_steps_per_second': 1.263, 'epoch': 2.0}
{'loss': 0.1274, 'learning_rate': 3.2e-06, 'epoch': 3.0}
{'eval_loss': 0.259367972612381, 'eval_f1': 0.6454545454545455, 'eval_roc_auc': 0.8116616288949868, 'eval_accuracy': 0.6, 'eval_runtime': 3.1786, 'eval_samples_per_second': 31.46, 'eval_steps_per_second': 1.258, 'epoch': 3.0}
{'loss': 0.0999, 'learning_rate': 1.6e-06, 'epoch': 4.0}
{'eval_loss': 0.2398892343044281, 'eval_f1': 0.6851851851851851, 'eval_roc_auc': 0.8312462564945468, 'eval_accuracy': 0.62, 'eval_runtime': 3.1664, 'eval_samples_per_second': 31.582, 'eval_steps_per_second': 1.263, 'epoch': 4.0}
{'loss': 0.0794, 'learning_rate': 0.0, 'epoch': 5.0}
{'eval_loss': 0.27396366000175476, 'eval_f1': 0.6363636363636364, 'eval_roc_auc': 0.8060898998481705, 'eval_accuracy': 0.56, 'eval_runtime': 3.1732, 'eval_samples_per_second': 31.514, 'eval_steps_per_second': 1.261, 'epoch': 5.0}
{'train_runtime': 4683.1619, 'train_samples_per_second': 9.378, 'train_steps_per_second': 1.172, 'train_loss': 0.14271886856829533, 'epoch': 5.0}
F1: 0.7263681592039802
              precision    recall  f1-score   support

          IN       0.71      0.50      0.59        10
          NA       0.57      1.00      0.73         4
          HI       0.63      0.80      0.71        15
          LY       0.76      0.88      0.82        33
          IP       0.00      0.00      0.00         0
          SP       0.88      0.61      0.72        36
          ID       0.00      0.00      0.00         4
          OP       1.00      1.00      1.00         1

   micro avg       0.74      0.71      0.73       103
   macro avg       0.57      0.60      0.57       103
weighted avg       0.75      0.71      0.71       103
 samples avg       0.70      0.72      0.71       103

END: to 4.8.2022 15.05.09 +0300
