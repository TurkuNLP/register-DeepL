learning rate: 8e-6 treshold: 0.4 batch: 8 epochs: 5
Namespace(batch=8, checkpoint='../multilabel/all', epochs=5, learning=8e-06, multilingual=True, saved='saved_models/all_multilingual', test_set=['test_sets/chi_all.tsv'], train_set=['AfterDeepL/chi_FINAL.tsv.gz', 'AfterDeepL/ja_FINAL.tsv.gz', 'AfterDeepL/es_FINAL.tsv.gz', 'AfterDeepL/pt_FINAL.tsv.gz', 'downsampled/all_downsampled.tsv.gz'], treshold=0.4)
{'dev': Dataset({
    features: ['text', 'label'],
    num_rows: 9259
}),
 'test': Dataset({
    features: ['text', 'label'],
    num_rows: 317
}),
 'train': Dataset({
    features: ['text', 'label'],
    num_rows: 37032
})}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
