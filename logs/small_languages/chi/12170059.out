learning rate: 5e-6 treshold: 0.3 batch: 7 epochs: 5
Namespace(batch=7, checkpoint='../multilabel/chi', epochs=5, lang='chi', learning=5e-06, multilingual=False, saved='saved_models/all_multilingual', test_set=['test_sets/chi_all.tsv'], train_set=['AfterDeepL/main_labels_only/chi_FINAL.modified.tsv.gz'], treshold=0.3)
{'dev': Dataset({
    features: ['text', 'label'],
    num_rows: 1752
}),
 'test': Dataset({
    features: ['text', 'label'],
    num_rows: 317
}),
 'train': Dataset({
    features: ['text', 'label'],
    num_rows: 7004
})}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.2752, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'eval_loss': 0.204784095287323, 'eval_f1': 0.7232081124683107, 'eval_roc_auc': 0.8468497119798566, 'eval_accuracy': 0.5490867579908676, 'eval_runtime': 53.3191, 'eval_samples_per_second': 32.859, 'eval_steps_per_second': 1.032, 'epoch': 1.0}
{'loss': 0.1891, 'learning_rate': 3e-06, 'epoch': 2.0}
{'eval_loss': 0.18974019587039948, 'eval_f1': 0.7408614668218858, 'eval_roc_auc': 0.8548799334332048, 'eval_accuracy': 0.5856164383561644, 'eval_runtime': 53.316, 'eval_samples_per_second': 32.861, 'eval_steps_per_second': 1.032, 'epoch': 2.0}
{'loss': 0.1588, 'learning_rate': 2.0000000000000003e-06, 'epoch': 3.0}
{'eval_loss': 0.19291450083255768, 'eval_f1': 0.7392978482446205, 'eval_roc_auc': 0.8613764546441492, 'eval_accuracy': 0.5776255707762558, 'eval_runtime': 53.3275, 'eval_samples_per_second': 32.854, 'eval_steps_per_second': 1.031, 'epoch': 3.0}
{'loss': 0.1346, 'learning_rate': 1.0000000000000002e-06, 'epoch': 4.0}
{'eval_loss': 0.19721558690071106, 'eval_f1': 0.7434651862132777, 'eval_roc_auc': 0.8582045821641752, 'eval_accuracy': 0.5901826484018264, 'eval_runtime': 53.2728, 'eval_samples_per_second': 32.887, 'eval_steps_per_second': 1.032, 'epoch': 4.0}
{'loss': 0.1171, 'learning_rate': 0.0, 'epoch': 5.0}
{'eval_loss': 0.2011369913816452, 'eval_f1': 0.7472222222222222, 'eval_roc_auc': 0.8602984753668356, 'eval_accuracy': 0.6044520547945206, 'eval_runtime': 54.1571, 'eval_samples_per_second': 32.35, 'eval_steps_per_second': 1.016, 'epoch': 5.0}
{'train_runtime': 3974.7546, 'train_samples_per_second': 8.811, 'train_steps_per_second': 1.259, 'train_loss': 0.1749566092476859, 'epoch': 5.0}
F1: 0.6443514644351463
              precision    recall  f1-score   support

          IN       0.24      0.58      0.34        12
          NA       0.50      0.88      0.64         8
          HI       0.49      0.87      0.63        68
          LY       0.82      0.65      0.72       115
          IP       0.50      0.67      0.57         3
          SP       0.80      0.72      0.76        97
          ID       0.38      0.27      0.32        37
          OP       0.33      1.00      0.50         1

   micro avg       0.61      0.68      0.64       341
   macro avg       0.51      0.70      0.56       341
weighted avg       0.67      0.68      0.65       341
 samples avg       0.63      0.69      0.64       341

