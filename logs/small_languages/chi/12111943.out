learning rate: 5e-6 treshold: 0.4 batch: 8 epochs: 5
Namespace(batch=8, checkpoint='../multilabel/chi', epochs=5, learning=5e-06, multilingual=False, saved='saved_models/all_multilingual', test_set=['test_sets/chi_all.tsv'], train_set=['AfterDeepL/chi_FINAL.tsv.gz'], treshold=0.4)
{'dev': Dataset({
    features: ['text', 'label'],
    num_rows: 1852
}),
 'test': Dataset({
    features: ['text', 'label'],
    num_rows: 317
}),
 'train': Dataset({
    features: ['text', 'label'],
    num_rows: 7405
})}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.2769, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.0}
{'eval_loss': 0.20726178586483002, 'eval_f1': 0.6871134020618557, 'eval_roc_auc': 0.8029162667706027, 'eval_accuracy': 0.5491360691144709, 'eval_runtime': 57.455, 'eval_samples_per_second': 32.234, 'eval_steps_per_second': 1.009, 'epoch': 1.0}
{'loss': 0.1923, 'learning_rate': 3e-06, 'epoch': 2.0}
{'eval_loss': 0.18757221102714539, 'eval_f1': 0.72686230248307, 'eval_roc_auc': 0.8312481765477667, 'eval_accuracy': 0.6069114470842333, 'eval_runtime': 59.3994, 'eval_samples_per_second': 31.179, 'eval_steps_per_second': 0.976, 'epoch': 2.0}
{'loss': 0.1604, 'learning_rate': 2.0000000000000003e-06, 'epoch': 3.0}
{'eval_loss': 0.18114309012889862, 'eval_f1': 0.7402882970925971, 'eval_roc_auc': 0.8455976454483168, 'eval_accuracy': 0.6187904967602592, 'eval_runtime': 67.467, 'eval_samples_per_second': 27.45, 'eval_steps_per_second': 0.86, 'epoch': 3.0}
{'loss': 0.1369, 'learning_rate': 1.0000000000000002e-06, 'epoch': 4.0}
{'eval_loss': 0.18273045122623444, 'eval_f1': 0.7412109375, 'eval_roc_auc': 0.846321234449764, 'eval_accuracy': 0.6220302375809935, 'eval_runtime': 64.9466, 'eval_samples_per_second': 28.516, 'eval_steps_per_second': 0.893, 'epoch': 4.0}
{'loss': 0.1202, 'learning_rate': 0.0, 'epoch': 5.0}
{'eval_loss': 0.1885448545217514, 'eval_f1': 0.7446808510638298, 'eval_roc_auc': 0.8509212836846306, 'eval_accuracy': 0.6295896328293736, 'eval_runtime': 67.7084, 'eval_samples_per_second': 27.353, 'eval_steps_per_second': 0.857, 'epoch': 5.0}
{'train_runtime': 5060.247, 'train_samples_per_second': 7.317, 'train_steps_per_second': 0.915, 'train_loss': 0.17732456610990652, 'epoch': 5.0}
F1: 0.5764895330112721
