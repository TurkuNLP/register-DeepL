learning rate: 8e-6 treshold: 0.4 batch: 7 epochs: 5
Namespace(batch=7, checkpoint='../multilabel/pt', epochs=5, full=False, lang='pt', learning=8e-06, model='neuralmind/bert-large-portuguese-cased', multilingual=False, saved='saved_models/all_multilingual', test_set=['test_sets/main_labels_only/pt_test_modified.tsv'], train_set=['AfterDeepL/main_labels_only/pt_FINAL.modified.tsv.gz'], treshold=0.4)
{'dev': Dataset({
    features: ['text', 'label'],
    num_rows: 1751
}),
 'test': Dataset({
    features: ['text', 'label'],
    num_rows: 332
}),
 'train': Dataset({
    features: ['text', 'label'],
    num_rows: 7003
})}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.3223, 'learning_rate': 6.4e-06, 'epoch': 1.0}
{'eval_loss': 0.23318791389465332, 'eval_f1': 0.6557808796418225, 'eval_roc_auc': 0.7798049949139829, 'eval_accuracy': 0.526556253569389, 'eval_runtime': 54.6783, 'eval_samples_per_second': 32.024, 'eval_steps_per_second': 1.006, 'epoch': 1.0}
{'loss': 0.1973, 'learning_rate': 4.8e-06, 'epoch': 2.0}
{'eval_loss': 0.20780670642852783, 'eval_f1': 0.7093911757456248, 'eval_roc_auc': 0.8237410649672463, 'eval_accuracy': 0.5745288406624786, 'eval_runtime': 54.6833, 'eval_samples_per_second': 32.021, 'eval_steps_per_second': 1.006, 'epoch': 2.0}
{'loss': 0.1524, 'learning_rate': 3.2e-06, 'epoch': 3.0}
{'eval_loss': 0.19978760182857513, 'eval_f1': 0.7345679012345678, 'eval_roc_auc': 0.847770443168101, 'eval_accuracy': 0.5859508852084523, 'eval_runtime': 54.6454, 'eval_samples_per_second': 32.043, 'eval_steps_per_second': 1.006, 'epoch': 3.0}
{'loss': 0.1225, 'learning_rate': 1.6e-06, 'epoch': 4.0}
{'eval_loss': 0.20899350941181183, 'eval_f1': 0.7309082148879787, 'eval_roc_auc': 0.8418476251283913, 'eval_accuracy': 0.5950885208452313, 'eval_runtime': 54.6257, 'eval_samples_per_second': 32.055, 'eval_steps_per_second': 1.007, 'epoch': 4.0}
{'loss': 0.1012, 'learning_rate': 0.0, 'epoch': 5.0}
{'eval_loss': 0.21841976046562195, 'eval_f1': 0.7275787448905988, 'eval_roc_auc': 0.840381806202548, 'eval_accuracy': 0.5939463163906339, 'eval_runtime': 55.7895, 'eval_samples_per_second': 31.386, 'eval_steps_per_second': 0.986, 'epoch': 5.0}
{'train_runtime': 3842.9724, 'train_samples_per_second': 9.111, 'train_steps_per_second': 1.302, 'train_loss': 0.17911491432151833, 'epoch': 5.0}
F1: 0.6751054852320675
              precision    recall  f1-score   support

          IN       0.76      0.41      0.53        32
          NA       0.38      0.18      0.24        17
          HI       0.43      0.47      0.45        55
          LY       0.77      0.75      0.76       113
          IP       0.00      0.00      0.00         4
          SP       0.79      0.85      0.82       113
          ID       0.57      0.57      0.57        30
          OP       0.00      0.00      0.00         0

   micro avg       0.69      0.66      0.68       364
   macro avg       0.46      0.40      0.42       364
weighted avg       0.68      0.66      0.66       364
 samples avg       0.68      0.68      0.67       364

END: ke 22.6.2022 10.48.20 +0300
