learning rate: 8e-6 treshold: 0.5 batch: 7 epochs: 5
Namespace(batch=7, epochs=5, learning=8e-06, multilingual=False, test_set=['test_sets/pt_test_modified.tsv'], train_set=['main_labels_only/pt_FINAL.modified.tsv.gz'], treshold=0.5)
Downloading and preparing dataset csv/default to /users/annieske/.cache/huggingface/datasets/csv/default-cd7c4cf5006041f9/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...
Dataset csv downloaded and prepared to /users/annieske/.cache/huggingface/datasets/csv/default-cd7c4cf5006041f9/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.
{'dev': Dataset({
    features: ['text', 'label'],
    num_rows: 1851
}),
 'test': Dataset({
    features: ['text', 'label'],
    num_rows: 332
}),
 'train': Dataset({
    features: ['text', 'label'],
    num_rows: 7404
})}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.2645, 'learning_rate': 6.4e-06, 'epoch': 1.0}
{'eval_loss': 0.2322613149881363, 'eval_f1': 0.6647058823529413, 'eval_roc_auc': 0.7908060525861574, 'eval_accuracy': 0.5783132530120482, 'eval_runtime': 10.5172, 'eval_samples_per_second': 31.567, 'eval_steps_per_second': 1.046, 'epoch': 1.0}
{'loss': 0.1772, 'learning_rate': 4.8e-06, 'epoch': 2.0}
{'eval_loss': 0.22732175886631012, 'eval_f1': 0.6685472496473908, 'eval_roc_auc': 0.8019892411253667, 'eval_accuracy': 0.5692771084337349, 'eval_runtime': 10.4476, 'eval_samples_per_second': 31.777, 'eval_steps_per_second': 1.053, 'epoch': 2.0}
{'loss': 0.1394, 'learning_rate': 3.2e-06, 'epoch': 3.0}
{'eval_loss': 0.24532431364059448, 'eval_f1': 0.6798866855524079, 'eval_roc_auc': 0.807419020769806, 'eval_accuracy': 0.6204819277108434, 'eval_runtime': 10.4596, 'eval_samples_per_second': 31.741, 'eval_steps_per_second': 1.052, 'epoch': 3.0}
{'loss': 0.1083, 'learning_rate': 1.6e-06, 'epoch': 4.0}
{'eval_loss': 0.2651011049747467, 'eval_f1': 0.6767955801104972, 'eval_roc_auc': 0.8114512015035575, 'eval_accuracy': 0.5903614457831325, 'eval_runtime': 10.4581, 'eval_samples_per_second': 31.746, 'eval_steps_per_second': 1.052, 'epoch': 4.0}
{'loss': 0.0855, 'learning_rate': 0.0, 'epoch': 5.0}
{'eval_loss': 0.27856895327568054, 'eval_f1': 0.6912568306010929, 'eval_roc_auc': 0.8224402124925685, 'eval_accuracy': 0.5903614457831325, 'eval_runtime': 10.4588, 'eval_samples_per_second': 31.744, 'eval_steps_per_second': 1.052, 'epoch': 5.0}
{'train_runtime': 4108.2449, 'train_samples_per_second': 9.011, 'train_steps_per_second': 1.288, 'train_loss': 0.154963343715848, 'epoch': 5.0}
F1: 0.6685472496473908
