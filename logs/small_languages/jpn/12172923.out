learning rate: 8e-6 treshold: 0.5 batch: 8 epochs: 5
Namespace(batch=8, checkpoint='../multilabel/jpn', epochs=5, lang='jpn', learning=8e-06, multilingual=False, saved='saved_models/all_multilingual', test_set=['test_sets/jpn_test.tsv'], train_set=['AfterDeepL/main_labels_only/ja_FINAL.modified.tsv.gz'], treshold=0.5)
{'dev': Dataset({
    features: ['text', 'label'],
    num_rows: 1749
}),
 'test': Dataset({
    features: ['text', 'label'],
    num_rows: 100
}),
 'train': Dataset({
    features: ['text', 'label'],
    num_rows: 6995
})}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.2676, 'learning_rate': 6.4e-06, 'epoch': 1.0}
{'eval_loss': 0.21950732171535492, 'eval_f1': 0.6910316226023846, 'eval_roc_auc': 0.7989607201783541, 'eval_accuracy': 0.5654659805603202, 'eval_runtime': 53.4155, 'eval_samples_per_second': 32.743, 'eval_steps_per_second': 1.03, 'epoch': 1.0}
{'loss': 0.1881, 'learning_rate': 4.8e-06, 'epoch': 2.0}
{'eval_loss': 0.19511426985263824, 'eval_f1': 0.7305054610109221, 'eval_roc_auc': 0.8249949694540454, 'eval_accuracy': 0.6123499142367067, 'eval_runtime': 53.3369, 'eval_samples_per_second': 32.792, 'eval_steps_per_second': 1.031, 'epoch': 2.0}
{'loss': 0.1493, 'learning_rate': 3.2e-06, 'epoch': 3.0}
{'eval_loss': 0.1994057595729828, 'eval_f1': 0.7346007604562738, 'eval_roc_auc': 0.827733984330274, 'eval_accuracy': 0.6209262435677531, 'eval_runtime': 53.3217, 'eval_samples_per_second': 32.801, 'eval_steps_per_second': 1.031, 'epoch': 3.0}
{'loss': 0.1197, 'learning_rate': 1.6e-06, 'epoch': 4.0}
{'eval_loss': 0.2032255381345749, 'eval_f1': 0.7474048442906575, 'eval_roc_auc': 0.8410999700864854, 'eval_accuracy': 0.62778730703259, 'eval_runtime': 53.3342, 'eval_samples_per_second': 32.793, 'eval_steps_per_second': 1.031, 'epoch': 4.0}
{'loss': 0.0988, 'learning_rate': 0.0, 'epoch': 5.0}
{'eval_loss': 0.20944100618362427, 'eval_f1': 0.745069033530572, 'eval_roc_auc': 0.8403997500612957, 'eval_accuracy': 0.6312178387650086, 'eval_runtime': 54.3227, 'eval_samples_per_second': 32.196, 'eval_steps_per_second': 1.012, 'epoch': 5.0}
{'train_runtime': 3953.0961, 'train_samples_per_second': 8.847, 'train_steps_per_second': 1.107, 'train_loss': 0.16471883893694197, 'epoch': 5.0}
F1: 0.6237623762376238
              precision    recall  f1-score   support

          IN       0.44      0.40      0.42        10
          NA       0.57      1.00      0.73         4
          HI       0.48      0.80      0.60        15
          LY       0.71      0.88      0.78        33
          IP       0.00      0.00      0.00         0
          SP       0.87      0.36      0.51        36
          ID       0.00      0.00      0.00         4
          OP       1.00      1.00      1.00         1

   micro avg       0.64      0.61      0.62       103
   macro avg       0.51      0.55      0.51       103
weighted avg       0.67      0.61      0.60       103
 samples avg       0.61      0.63      0.62       103

