learning rate: 8e-6 treshold: 0.5 batch: 8 epochs: 5
Namespace(batch=8, checkpoint='../multilabel/jpn', epochs=5, learning=8e-06, multilingual=False, saved='saved_models/all_multilingual', test_set=['test_sets/jpn_test.tsv'], train_set=['AfterDeepL/ja_FINAL.tsv.gz'], treshold=0.5)
{'dev': Dataset({
    features: ['text', 'label'],
    num_rows: 1848
}),
 'test': Dataset({
    features: ['text', 'label'],
    num_rows: 100
}),
 'train': Dataset({
    features: ['text', 'label'],
    num_rows: 7392
})}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.2678, 'learning_rate': 6.4e-06, 'epoch': 1.0}
{'eval_loss': 0.2061941921710968, 'eval_f1': 0.6699002964160603, 'eval_roc_auc': 0.7824794971515759, 'eval_accuracy': 0.5546536796536796, 'eval_runtime': 57.8515, 'eval_samples_per_second': 31.944, 'eval_steps_per_second': 1.003, 'epoch': 1.0}
{'loss': 0.1856, 'learning_rate': 4.8e-06, 'epoch': 2.0}
{'eval_loss': 0.20007364451885223, 'eval_f1': 0.7067970905442689, 'eval_roc_auc': 0.8178596601792807, 'eval_accuracy': 0.571969696969697, 'eval_runtime': 57.6289, 'eval_samples_per_second': 32.067, 'eval_steps_per_second': 1.006, 'epoch': 2.0}
{'loss': 0.1496, 'learning_rate': 3.2e-06, 'epoch': 3.0}
{'eval_loss': 0.2024223655462265, 'eval_f1': 0.7150644202180377, 'eval_roc_auc': 0.8254028011576129, 'eval_accuracy': 0.5936147186147186, 'eval_runtime': 110.9821, 'eval_samples_per_second': 16.651, 'eval_steps_per_second': 0.523, 'epoch': 3.0}
{'loss': 0.1191, 'learning_rate': 1.6e-06, 'epoch': 4.0}
{'eval_loss': 0.20831233263015747, 'eval_f1': 0.7231749142577167, 'eval_roc_auc': 0.8327854850613219, 'eval_accuracy': 0.5979437229437229, 'eval_runtime': 57.6303, 'eval_samples_per_second': 32.066, 'eval_steps_per_second': 1.006, 'epoch': 4.0}
{'loss': 0.0979, 'learning_rate': 0.0, 'epoch': 5.0}
{'eval_loss': 0.2140263468027115, 'eval_f1': 0.7242798353909465, 'eval_roc_auc': 0.8364279021170383, 'eval_accuracy': 0.5979437229437229, 'eval_runtime': 89.414, 'eval_samples_per_second': 20.668, 'eval_steps_per_second': 0.649, 'epoch': 5.0}
{'train_runtime': 4356.4029, 'train_samples_per_second': 8.484, 'train_steps_per_second': 1.061, 'train_loss': 0.16397878143178435, 'epoch': 5.0}
F1: 0.6632124352331606
