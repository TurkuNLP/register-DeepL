learning rate: 8e-6 treshold: 0.4 batch: 8 epochs: 5
Namespace(batch=8, checkpoint='../multilabel/pt', epochs=5, full=False, lang='pt', learning=8e-06, model='xlm-roberta-large', multilingual=False, saved='saved_models/all_multilingual', test_set=['data/test_sets/main_labels_only/pt_test_modified.tsv'], train_set=['data/AfterDeepL/main_labels_only/pt_FINAL.modified.tsv.gz'], treshold=0.4)
Downloading and preparing dataset csv/default to /users/annieske/.cache/huggingface/datasets/csv/default-4ced0503909a1a75/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...
Dataset csv downloaded and prepared to /users/annieske/.cache/huggingface/datasets/csv/default-4ced0503909a1a75/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.
{'test': Dataset({
    features: ['text', 'label'],
    num_rows: 332
}),
 'train': Dataset({
    features: ['text', 'label'],
    num_rows: 8754
})}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.2563, 'learning_rate': 6.4e-06, 'epoch': 1.0}
{'eval_loss': 0.22182929515838623, 'eval_f1': 0.6789473684210526, 'eval_roc_auc': 0.8242908923537197, 'eval_accuracy': 0.5421686746987951, 'eval_runtime': 10.9325, 'eval_samples_per_second': 30.368, 'eval_steps_per_second': 1.006, 'epoch': 1.0}
{'loss': 0.1719, 'learning_rate': 4.8e-06, 'epoch': 2.0}
{'eval_loss': 0.2293364703655243, 'eval_f1': 0.6703296703296703, 'eval_roc_auc': 0.8089868246936309, 'eval_accuracy': 0.5783132530120482, 'eval_runtime': 10.6537, 'eval_samples_per_second': 31.163, 'eval_steps_per_second': 1.033, 'epoch': 2.0}
{'loss': 0.1374, 'learning_rate': 3.2e-06, 'epoch': 3.0}
{'eval_loss': 0.23221854865550995, 'eval_f1': 0.680327868852459, 'eval_roc_auc': 0.8160731066490229, 'eval_accuracy': 0.5843373493975904, 'eval_runtime': 10.5354, 'eval_samples_per_second': 31.513, 'eval_steps_per_second': 1.044, 'epoch': 3.0}
{'loss': 0.1091, 'learning_rate': 1.6e-06, 'epoch': 4.0}
{'eval_loss': 0.27002832293510437, 'eval_f1': 0.6829931972789116, 'eval_roc_auc': 0.8186022093090156, 'eval_accuracy': 0.5903614457831325, 'eval_runtime': 10.7719, 'eval_samples_per_second': 30.821, 'eval_steps_per_second': 1.021, 'epoch': 4.0}
{'loss': 0.088, 'learning_rate': 0.0, 'epoch': 5.0}
{'eval_loss': 0.28723087906837463, 'eval_f1': 0.6746666666666667, 'eval_roc_auc': 0.8185135109218877, 'eval_accuracy': 0.5692771084337349, 'eval_runtime': 10.6765, 'eval_samples_per_second': 31.096, 'eval_steps_per_second': 1.03, 'epoch': 5.0}
{'train_runtime': 5151.0576, 'train_samples_per_second': 8.497, 'train_steps_per_second': 1.063, 'train_loss': 0.1525322859254602, 'epoch': 5.0}
F1: 0.6789473684210526
              precision    recall  f1-score   support

          IN       0.74      0.44      0.55        32
          NA       0.67      0.24      0.35        17
          HI       0.38      0.78      0.51        55
          LY       0.80      0.76      0.78       113
          IP       0.00      0.00      0.00         4
          SP       0.76      0.88      0.81       113
          ID       0.63      0.40      0.49        30
          OP       0.00      0.00      0.00         0

   micro avg       0.65      0.71      0.68       364
   macro avg       0.50      0.44      0.44       364
weighted avg       0.69      0.71      0.68       364
 samples avg       0.68      0.73      0.69       364

END: to 4.8.2022 15.15.54 +0300
