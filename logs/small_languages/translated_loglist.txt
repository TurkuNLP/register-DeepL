ORIGINALLY THESE WERE THE SETTINGS 
    eval_steps=100,
    logging_steps=100,
    learning_rate=0.00001,
    per_device_train_batch_size=8,
    max_steps=1500

    treshold 0.3


ALL THE GOOD NUMBERS ARE FALSE BECAUSE THE TEST SET WAS SPLICED WRONG!!!!!! :(



0.4 treshold seems to be the best for now
what about using the test set as "dev" and not testing at all? will it learn better?



PORTUGUESE


12044723
--batch 7 --treshold 0.4 --epochs 5 --learning 8e-6 
'eval_f1': 0.7516594186312657
F1: 0.6922024623803009

12067618
--batch 8 --treshold 0.3 --epochs 5 --learning 8e-6
'eval_f1': 0.7477378062237917
F1: 0.6640726329442284

test with steps
12050641
F1: 0.6700507614213198
=> theory is that there is something going on with the shuffling since it worked previously

12055899
test with default settings and possibly proper shuffling?
F1: 0.6676056338028169
DID NOT HELP...

12056283
--batch 8 --treshold 0.3 --epochs 5 --learning 8e-6
F1: 0.6666666666666666

what about using the test set as "dev" and not testing at all?
12061778
F1: 0.6972222222222222


12076034
--batch 8 --treshold 0.4 --epochs 5 --learning 8e-6
F1: 0.6648721399730821

12077636
--batch 8 --treshold 0.5 --epochs 5 --learning 1e-6
F1: 0.6424242424242425

12077637
--batch 8 --treshold 0.4 --epochs 5 --learning 1e-6
F1: 0.6912181303116146

12077638
--batch 7 --treshold 0.4 --epochs 5 --learning 1e-6
F1: 0.6722925457102672

12077641
--batch 7 --treshold 0.5 --epochs 5 --learning 1e-6
F1: 0.6666666666666667

12078275
--batch 7 --treshold 0.4 --epochs 5 --learning 4e-6
F1: 0.6656891495601173





SPANISH

12045641
--batch 7 --treshold 0.4 --epochs 5 --learning 8e-6 
'eval_f1': 0.7378551787351054
F1: 0.6728110599078341

12047632
--batch 8 --treshold 0.3 --epochs 5 --learning 8e-6
F1: 0.6636363636363636

12076049
--batch 8 --treshold 0.4 --epochs 5 --learning 8e-6
F1: 0.6965174129353234


12078424
learning rate: 4e-6 treshold: 0.3 batch: 8 epochs: 5
F1: 0.6754385964912281

12078425
learning rate: 4e-6 treshold: 0.3 batch: 7 epochs: 5
F1: 0.6548672566371682

12095005
learning rate: 5e-06 treshold: 0.4 batch: 8 epochs: 5
F1: 0.6701570680628272

12095013
learning rate: 7e-5 treshold: 0.4 batch: 8 epochs: 5
F1: 0.66

12095015
learning rate: 1e-5 treshold: 0.4 batch: 8 epochs: 5
F1: 0.6701570680628272




JAPANESE

12045673
--batch 7 --treshold 0.4 --epochs 5 --learning 8e-6 
'eval_f1': 0.7506297229219143
F1: 0.6542056074766356

12048128
--batch 8 --treshold 0.3 --epochs 5 --learning 8e-6
F1: 0.6311111111111111

12076056
--batch 8 --treshold 0.4 --epochs 5 --learning 8e-6
F1: 0.6926829268292684

12078427
learning rate: 4e-6 treshold: 0.3 batch: 7 epochs: 5
F1: 0.6728971962616822

12978428
learning rate: 4e-6 treshold: 0.3 batch: 8 epochs: 5
F1: 0.6666666666666666

1e-5 => F1 0.0

12095026
learning rate: 4e-6 treshold: 0.4 batch: 8 epochs: 5
F1: 0.6788990825688073

12095027
learning rate: 5e-6 treshold: 0.4 batch: 8 epochs: 5
F1: 0.6788990825688073

12095028
learning rate: 7e-5 treshold: 0.4 batch: 8 epochs: 5
F1: 0.6146341463414634




CHINESE

12045680
--batch 7 --treshold 0.4 --epochs 5 --learning 8e-6 
'eval_f1': 0.7494871210394348
F1: 0.5557206537890045

12048131
--batch 8 --treshold 0.3 --epochs 5 --learning 8e-6
F1: 0.5356125356125356

12073149
--batch 8 --treshold 0.4 --epochs 5 --learning 8e-6
F1: 0.6259314456035767
=> this is interesting because the eval F1 kept going down but this was better :D

12078321
F1: 0.6277777777777778
Namespace(batch=7, epochs=5, learning=4e-06, treshold=0.3)

12078347
F1: 0.6592489568845619
learning rate: 4e-6 treshold: 0.3 batch: 8 epochs: 5

12095099
learning rate: 4e-6 treshold: 0.4 batch: 8 epochs: 5
F1: 0.632375189107413







MULTILINGUAL SHOULD RUN ONCE MORE AND SAVE THE MODEL, THEN CAN TEST ON OTHER TEST SETS

CHINESE TEST SET
12047169
--batch 7 --treshold 0.4 --epochs 5 --learning 8e-6 
'eval_f1': 0.9488701209769459
F1: 0.5674157303370787


SPANISH TEST SET
12049956
--batch 8 --treshold 0.3 --epochs 5 --learning 8e-6
F1: 0.6812227074235807

PORTUGUESE TEST SET
12049960
--batch 8 --treshold 0.3 --epochs 5 --learning 8e-6
F1: 0.6459143968871596


JAPANESE TEST SET
12050240
--batch 8 --treshold 0.3 --epochs 5 --learning 8e-6
F1: 0.635593220338983


BASED ON MODEL FROM 12058846










MULTILINGUAL TRANSFER

12077295 based on downsampled model  12073502

SPANISH
F1: 0.6804123711340205

PORTUGUESE
F1: 0.7010309278350515

JAPANESE
F1: 0.6321243523316062


CHINESE
F1: 0.6873065015479876