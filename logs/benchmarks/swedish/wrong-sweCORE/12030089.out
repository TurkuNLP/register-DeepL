Downloading and preparing dataset csv/default to /users/annieske/.cache/huggingface/datasets/csv/default-e6b63fbcea6fdf73/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...
Dataset csv downloaded and prepared to /users/annieske/.cache/huggingface/datasets/csv/default-e6b63fbcea6fdf73/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.
['IN', None, 'NA OP', 'ID', 'NA OP']
[['IN'], ['NA'], ['NA', 'OP'], ['ID'], ['NA', 'OP']]
[[0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0],
 [0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0]]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.2689, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'eval_loss': 0.23816590011119843, 'eval_f1': 0.6787925696594428, 'eval_roc_auc': 0.7950535596693026, 'eval_accuracy': 0.5760407816482583, 'eval_runtime': 38.5568, 'eval_samples_per_second': 30.526, 'eval_steps_per_second': 0.96, 'epoch': 1.0}
{'loss': 0.1995, 'learning_rate': 2.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.1986183226108551, 'eval_f1': 0.7361268403171007, 'eval_roc_auc': 0.8322610489732861, 'eval_accuracy': 0.622769753610875, 'eval_runtime': 38.6412, 'eval_samples_per_second': 30.46, 'eval_steps_per_second': 0.958, 'epoch': 2.0}
{'loss': 0.1462, 'learning_rate': 2e-05, 'epoch': 3.0}
{'eval_loss': 0.20075629651546478, 'eval_f1': 0.7408720311946119, 'eval_roc_auc': 0.8510111361617817, 'eval_accuracy': 0.6006796941376381, 'eval_runtime': 39.6567, 'eval_samples_per_second': 29.68, 'eval_steps_per_second': 0.933, 'epoch': 3.0}
{'loss': 0.1026, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.19087554514408112, 'eval_f1': 0.7804347826086957, 'eval_roc_auc': 0.8682854574597045, 'eval_accuracy': 0.6618521665250637, 'eval_runtime': 38.4356, 'eval_samples_per_second': 30.623, 'eval_steps_per_second': 0.963, 'epoch': 4.0}
{'loss': 0.0754, 'learning_rate': 6.666666666666667e-06, 'epoch': 5.0}
{'eval_loss': 0.1908465325832367, 'eval_f1': 0.7866049162807268, 'eval_roc_auc': 0.8767232470820023, 'eval_accuracy': 0.6652506372132541, 'eval_runtime': 38.6788, 'eval_samples_per_second': 30.43, 'eval_steps_per_second': 0.957, 'epoch': 5.0}
{'loss': 0.0493, 'learning_rate': 0.0, 'epoch': 6.0}
{'eval_loss': 0.20016023516654968, 'eval_f1': 0.7943760984182776, 'eval_roc_auc': 0.8853009854177911, 'eval_accuracy': 0.6610025488530161, 'eval_runtime': 38.5255, 'eval_samples_per_second': 30.551, 'eval_steps_per_second': 0.96, 'epoch': 6.0}
{'train_runtime': 1658.1149, 'train_samples_per_second': 6.995, 'train_steps_per_second': 1.002, 'train_loss': 0.14032738877260872, 'epoch': 6.0}
F1: 0.7688984881209503

SWE rerun with the changed encoding and dataset building but same parametres
