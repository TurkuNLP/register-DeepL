8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.3457, 'learning_rate': 6.666666666666667e-06, 'epoch': 1.0}
{'eval_loss': 0.26065000891685486, 'eval_f1': 0.6529968454258674, 'eval_roc_auc': 0.8009420936084329, 'eval_accuracy': 0.4606598984771574, 'eval_runtime': 24.5079, 'eval_samples_per_second': 32.153, 'eval_steps_per_second': 1.02, 'epoch': 1.0}
{'loss': 0.2251, 'learning_rate': 5.333333333333333e-06, 'epoch': 2.0}
{'eval_loss': 0.20178036391735077, 'eval_f1': 0.7322287546766434, 'eval_roc_auc': 0.8441542357928801, 'eval_accuracy': 0.600253807106599, 'eval_runtime': 24.4584, 'eval_samples_per_second': 32.218, 'eval_steps_per_second': 1.022, 'epoch': 2.0}
{'loss': 0.1726, 'learning_rate': 4e-06, 'epoch': 3.0}
{'eval_loss': 0.19496120512485504, 'eval_f1': 0.7389686337054759, 'eval_roc_auc': 0.8495248050732238, 'eval_accuracy': 0.5926395939086294, 'eval_runtime': 24.4315, 'eval_samples_per_second': 32.253, 'eval_steps_per_second': 1.023, 'epoch': 3.0}
{'loss': 0.1402, 'learning_rate': 2.6666666666666664e-06, 'epoch': 4.0}
{'eval_loss': 0.1926906853914261, 'eval_f1': 0.7431865828092243, 'eval_roc_auc': 0.8558338495996713, 'eval_accuracy': 0.5850253807106599, 'eval_runtime': 24.4453, 'eval_samples_per_second': 32.235, 'eval_steps_per_second': 1.023, 'epoch': 4.0}
{'loss': 0.1146, 'learning_rate': 1.3333333333333332e-06, 'epoch': 5.0}
{'eval_loss': 0.1972140073776245, 'eval_f1': 0.7440381558028616, 'eval_roc_auc': 0.853377261451467, 'eval_accuracy': 0.6040609137055838, 'eval_runtime': 24.4148, 'eval_samples_per_second': 32.275, 'eval_steps_per_second': 1.024, 'epoch': 5.0}
{'loss': 0.0985, 'learning_rate': 0.0, 'epoch': 6.0}
{'eval_loss': 0.19528256356716156, 'eval_f1': 0.7566999474513925, 'eval_roc_auc': 0.8632304019200909, 'eval_accuracy': 0.6040609137055838, 'eval_runtime': 24.4306, 'eval_samples_per_second': 32.255, 'eval_steps_per_second': 1.023, 'epoch': 6.0}
{'train_runtime': 1403.4827, 'train_samples_per_second': 8.23, 'train_steps_per_second': 1.03, 'train_loss': 0.18277684814537545, 'epoch': 6.0}
F1: 0.7429171038824764

FreCore with 6 epochs, batch 8, some other hyperparameter has to be changed to make the results better?