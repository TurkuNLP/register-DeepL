8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.3101, 'learning_rate': 5.333333333333333e-06, 'epoch': 1.0}
{'eval_loss': 0.22192798554897308, 'eval_f1': 0.7211267605633804, 'eval_roc_auc': 0.8290072430875706, 'eval_accuracy': 0.5848329048843187, 'eval_runtime': 23.91, 'eval_samples_per_second': 32.539, 'eval_steps_per_second': 1.046, 'epoch': 1.0}
{'loss': 0.1927, 'learning_rate': 2.6666666666666664e-06, 'epoch': 2.0}
{'eval_loss': 0.2049214243888855, 'eval_f1': 0.7322342239909039, 'eval_roc_auc': 0.8330766293344533, 'eval_accuracy': 0.622107969151671, 'eval_runtime': 23.8665, 'eval_samples_per_second': 32.598, 'eval_steps_per_second': 1.047, 'epoch': 2.0}
{'loss': 0.1531, 'learning_rate': 0.0, 'epoch': 3.0}
{'eval_loss': 0.18407845497131348, 'eval_f1': 0.7628524046434493, 'eval_roc_auc': 0.8578365225768799, 'eval_accuracy': 0.6388174807197944, 'eval_runtime': 23.8716, 'eval_samples_per_second': 32.591, 'eval_steps_per_second': 1.047, 'epoch': 3.0}
{'train_runtime': 709.6996, 'train_samples_per_second': 8.171, 'train_steps_per_second': 1.171, 'train_loss': 0.2186681469711873, 'epoch': 3.0}
F1: 0.7625899280575541

SweCore
This is worse than the previous results
batch 7, epoch 3, learning rate 8e-6