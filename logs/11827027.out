Downloading and preparing dataset json/default to /users/annieske/.cache/huggingface/datasets/json/default-8e89d03464b720dd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
Dataset json downloaded and prepared to /users/annieske/.cache/huggingface/datasets/json/default-8e89d03464b720dd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 2.6247, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.02}
{'eval_loss': 2.1557838916778564, 'eval_f1': 0.3686274509803922, 'eval_precision': 0.3686274509803922, 'eval_recall': 0.3686274509803922, 'eval_runtime': 47.5623, 'eval_samples_per_second': 101.866, 'eval_steps_per_second': 3.196, 'epoch': 0.02}
{'loss': 2.0755, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.04}
{'eval_loss': 1.9525138139724731, 'eval_f1': 0.4524251805985552, 'eval_precision': 0.4524251805985552, 'eval_recall': 0.4524251805985552, 'eval_runtime': 47.9858, 'eval_samples_per_second': 100.967, 'eval_steps_per_second': 3.168, 'epoch': 0.04}
{'loss': 1.9353, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.06}
{'eval_loss': 1.779968023300171, 'eval_f1': 0.509391124871001, 'eval_precision': 0.509391124871001, 'eval_recall': 0.509391124871001, 'eval_runtime': 48.1162, 'eval_samples_per_second': 100.694, 'eval_steps_per_second': 3.159, 'epoch': 0.06}
{'loss': 1.8427, 'learning_rate': 7.333333333333333e-06, 'epoch': 0.08}
{'eval_loss': 1.7323638200759888, 'eval_f1': 0.4996904024767802, 'eval_precision': 0.4996904024767802, 'eval_recall': 0.4996904024767802, 'eval_runtime': 48.0334, 'eval_samples_per_second': 100.867, 'eval_steps_per_second': 3.164, 'epoch': 0.08}
{'loss': 1.6875, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.1}
{'eval_loss': 1.676154375076294, 'eval_f1': 0.5190918472652218, 'eval_precision': 0.5190918472652218, 'eval_recall': 0.5190918472652218, 'eval_runtime': 48.082, 'eval_samples_per_second': 100.765, 'eval_steps_per_second': 3.161, 'epoch': 0.1}
{'loss': 1.6647, 'learning_rate': 6e-06, 'epoch': 0.12}
{'eval_loss': 1.6126517057418823, 'eval_f1': 0.5562435500515995, 'eval_precision': 0.5562435500515995, 'eval_recall': 0.5562435500515995, 'eval_runtime': 48.0041, 'eval_samples_per_second': 100.929, 'eval_steps_per_second': 3.166, 'epoch': 0.12}
{'loss': 1.6158, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.14}
{'eval_loss': 1.6410727500915527, 'eval_f1': 0.5370485036119711, 'eval_precision': 0.5370485036119711, 'eval_recall': 0.5370485036119711, 'eval_runtime': 48.048, 'eval_samples_per_second': 100.837, 'eval_steps_per_second': 3.164, 'epoch': 0.14}
{'loss': 1.611, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.17}
{'eval_loss': 1.6100337505340576, 'eval_f1': 0.5450980392156862, 'eval_precision': 0.5450980392156862, 'eval_recall': 0.5450980392156862, 'eval_runtime': 48.0149, 'eval_samples_per_second': 100.906, 'eval_steps_per_second': 3.166, 'epoch': 0.17}
{'loss': 1.574, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.19}
{'eval_loss': 1.5779027938842773, 'eval_f1': 0.554592363261094, 'eval_precision': 0.554592363261094, 'eval_recall': 0.554592363261094, 'eval_runtime': 48.0225, 'eval_samples_per_second': 100.89, 'eval_steps_per_second': 3.165, 'epoch': 0.19}
{'loss': 1.5519, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.21}
{'eval_loss': 1.476249098777771, 'eval_f1': 0.5940144478844169, 'eval_precision': 0.5940144478844169, 'eval_recall': 0.5940144478844169, 'eval_runtime': 48.0303, 'eval_samples_per_second': 100.874, 'eval_steps_per_second': 3.165, 'epoch': 0.21}
{'loss': 1.5155, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.23}
{'eval_loss': 1.4844244718551636, 'eval_f1': 0.5810113519091847, 'eval_precision': 0.5810113519091847, 'eval_recall': 0.5810113519091847, 'eval_runtime': 47.98, 'eval_samples_per_second': 100.98, 'eval_steps_per_second': 3.168, 'epoch': 0.23}
{'loss': 1.4526, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.25}
{'eval_loss': 1.4512360095977783, 'eval_f1': 0.5905056759545924, 'eval_precision': 0.5905056759545924, 'eval_recall': 0.5905056759545924, 'eval_runtime': 48.0084, 'eval_samples_per_second': 100.92, 'eval_steps_per_second': 3.166, 'epoch': 0.25}
{'loss': 1.5279, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.27}
{'eval_loss': 1.4450486898422241, 'eval_f1': 0.591124871001032, 'eval_precision': 0.591124871001032, 'eval_recall': 0.591124871001032, 'eval_runtime': 48.0344, 'eval_samples_per_second': 100.865, 'eval_steps_per_second': 3.164, 'epoch': 0.27}
{'loss': 1.4489, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.29}
{'eval_loss': 1.4367687702178955, 'eval_f1': 0.5973168214654283, 'eval_precision': 0.5973168214654283, 'eval_recall': 0.5973168214654283, 'eval_runtime': 48.0263, 'eval_samples_per_second': 100.882, 'eval_steps_per_second': 3.165, 'epoch': 0.29}
{'loss': 1.4036, 'learning_rate': 0.0, 'epoch': 0.31}
{'eval_loss': 1.4362956285476685, 'eval_f1': 0.5987616099071208, 'eval_precision': 0.5987616099071208, 'eval_recall': 0.5987616099071208, 'eval_runtime': 48.0028, 'eval_samples_per_second': 100.932, 'eval_steps_per_second': 3.166, 'epoch': 0.31}
{'train_runtime': 1143.1018, 'train_samples_per_second': 10.498, 'train_steps_per_second': 1.312, 'train_loss': 1.702097869873047, 'epoch': 0.31}
F1: 0.5721362229102167
DOWNSAMPLED ENGLISH WITH XLMR LARGE(?)
