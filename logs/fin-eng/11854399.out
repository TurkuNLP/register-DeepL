8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.2233, 'learning_rate': 6.666666666666667e-06, 'epoch': 1.0}
{'eval_loss': 0.18390285968780518, 'eval_f1': 0.7172031844929041, 'eval_roc_auc': 0.8560663573725648, 'eval_accuracy': 0.548213916993599, 'eval_runtime': 48.2061, 'eval_samples_per_second': 100.464, 'eval_steps_per_second': 3.153, 'epoch': 1.0}
{'loss': 0.1701, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.0}
{'eval_loss': 0.17451585829257965, 'eval_f1': 0.733035560900189, 'eval_roc_auc': 0.8681028982520944, 'eval_accuracy': 0.562874251497006, 'eval_runtime': 48.2403, 'eval_samples_per_second': 100.393, 'eval_steps_per_second': 3.151, 'epoch': 2.0}
{'loss': 0.1507, 'learning_rate': 0.0, 'epoch': 3.0}
{'eval_loss': 0.17473839223384857, 'eval_f1': 0.7406760415756833, 'eval_roc_auc': 0.8680682836235206, 'eval_accuracy': 0.5857939293826141, 'eval_runtime': 48.2852, 'eval_samples_per_second': 100.3, 'eval_steps_per_second': 3.148, 'epoch': 3.0}
{'train_runtime': 3622.24, 'train_samples_per_second': 28.089, 'train_steps_per_second': 3.512, 'train_loss': 0.18137726213947034, 'epoch': 3.0}
F1: 0.7331615120274914

DECENT RESULTS WHICH BASICALLY MATCH THE ONES VERONIKA HAS, MAYBE A BIT LOWER
MULTILABEL XLMR-BASE WITH ORIGINAL ENGLISH DATASET