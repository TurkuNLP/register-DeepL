Downloading and preparing dataset json/default to /users/annieske/.cache/huggingface/datasets/json/default-c894c614cb5ed0c3/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...
Dataset json downloaded and prepared to /users/annieske/.cache/huggingface/datasets/json/default-c894c614cb5ed0c3/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 1.3883, 'learning_rate': 6.666666666666667e-06, 'epoch': 1.0}
{'eval_loss': 1.1652812957763672, 'eval_f1': 0.6625386996904025, 'eval_precision': 0.6625386996904025, 'eval_recall': 0.6625386996904025, 'eval_runtime': 48.5382, 'eval_samples_per_second': 99.818, 'eval_steps_per_second': 3.132, 'epoch': 1.0}
{'loss': 1.0932, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.0}
{'eval_loss': 1.107838749885559, 'eval_f1': 0.6687306501547987, 'eval_precision': 0.6687306501547987, 'eval_recall': 0.6687306501547987, 'eval_runtime': 48.3607, 'eval_samples_per_second': 100.185, 'eval_steps_per_second': 3.143, 'epoch': 2.0}
{'loss': 0.9808, 'learning_rate': 0.0, 'epoch': 3.0}
{'eval_loss': 1.1153305768966675, 'eval_f1': 0.6736842105263158, 'eval_precision': 0.6736842105263158, 'eval_recall': 0.6736842105263158, 'eval_runtime': 51.7948, 'eval_samples_per_second': 93.542, 'eval_steps_per_second': 2.935, 'epoch': 3.0}
{'train_runtime': 4409.9574, 'train_samples_per_second': 26.369, 'train_steps_per_second': 3.297, 'train_loss': 1.154102341707938, 'epoch': 3.0}
F1: 0.6565531475748194
ALL OF ENGLISH DATA WITH XLMR BASE