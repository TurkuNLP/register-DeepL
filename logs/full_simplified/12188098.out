learning rate: 8e-6 treshold: 0.4 batch: 7 epochs: 5
Namespace(batch=7, checkpoint='../multilabel/pt', epochs=5, full=True, lang='pt', learning=8e-06, model='xlm-roberta-large', multilingual=False, saved='saved_models/all_multilingual', test_set=['test_sets/pt_test_modified.tsv'], train_set=['AfterDeepL/full_labels/pt_FINAL_full.tsv.gz'], treshold=0.4)
{'dev': Dataset({
    features: ['text', 'label'],
    num_rows: 1751
}),
 'test': Dataset({
    features: ['text', 'label'],
    num_rows: 332
}),
 'train': Dataset({
    features: ['text', 'label'],
    num_rows: 7003
})}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.1199, 'learning_rate': 6.4e-06, 'epoch': 1.0}
{'eval_loss': 0.0765414834022522, 'eval_f1': 0.707684452387031, 'eval_roc_auc': 0.82635017752542, 'eval_accuracy': 0.5859508852084523, 'eval_runtime': 54.21, 'eval_samples_per_second': 32.3, 'eval_steps_per_second': 1.015, 'epoch': 1.0}
{'loss': 0.0655, 'learning_rate': 4.8e-06, 'epoch': 2.0}
{'eval_loss': 0.06761340796947479, 'eval_f1': 0.7422428536525776, 'eval_roc_auc': 0.8576604751510463, 'eval_accuracy': 0.6133637921187892, 'eval_runtime': 53.1863, 'eval_samples_per_second': 32.922, 'eval_steps_per_second': 1.034, 'epoch': 2.0}
{'loss': 0.0508, 'learning_rate': 3.2e-06, 'epoch': 3.0}
{'eval_loss': 0.06465256214141846, 'eval_f1': 0.7485015583792856, 'eval_roc_auc': 0.867267225261568, 'eval_accuracy': 0.6190748143917761, 'eval_runtime': 53.2189, 'eval_samples_per_second': 32.902, 'eval_steps_per_second': 1.033, 'epoch': 3.0}
{'loss': 0.0403, 'learning_rate': 1.6e-06, 'epoch': 4.0}
{'eval_loss': 0.06553160399198532, 'eval_f1': 0.7549445248432224, 'eval_roc_auc': 0.8685881605971854, 'eval_accuracy': 0.6316390633923472, 'eval_runtime': 54.3216, 'eval_samples_per_second': 32.234, 'eval_steps_per_second': 1.012, 'epoch': 4.0}
{'loss': 0.0329, 'learning_rate': 0.0, 'epoch': 5.0}
{'eval_loss': 0.0676131546497345, 'eval_f1': 0.7580412866058569, 'eval_roc_auc': 0.8718655308263336, 'eval_accuracy': 0.6384922901199315, 'eval_runtime': 53.2367, 'eval_samples_per_second': 32.891, 'eval_steps_per_second': 1.033, 'epoch': 5.0}
{'train_runtime': 3943.2451, 'train_samples_per_second': 8.88, 'train_steps_per_second': 1.269, 'train_loss': 0.06188059605799474, 'epoch': 5.0}
F1: 0.49317738791422994
              precision    recall  f1-score   support

          HI       0.71      0.31      0.43        32
          ID       0.71      0.29      0.42        17
          IN       0.37      0.73      0.49        55
          IP       0.82      0.71      0.76       113
          LY       0.00      0.00      0.00         4
          NA       0.78      0.91      0.84       113
          OP       0.62      0.50      0.56        30
          SP       0.00      0.00      0.00         0
          av       0.00      0.00      0.00         6
          ds       0.00      0.00      0.00       106
         dtp       0.00      0.00      0.00        19
          ed       0.00      0.00      0.00         4
          en       0.00      0.00      0.00         3
          fi       0.00      0.00      0.00         1
          it       0.00      0.00      0.00         0
          lt       0.00      0.00      0.00         8
          nb       0.00      0.00      0.00        19
          ne       0.00      0.00      0.00        75
          ob       0.00      0.00      0.00         5
          ra       0.00      0.00      0.00         2
          re       0.00      0.00      0.00         6
          rs       0.00      0.00      0.00         9
          rv       0.00      0.00      0.00         9
          sr       0.00      0.00      0.00         6

   micro avg       0.66      0.39      0.49       642
   macro avg       0.17      0.14      0.15       642
weighted avg       0.40      0.39      0.38       642
 samples avg       0.69      0.41      0.49       642

END: ti 21.6.2022 16.54.03 +0300
