learning rate: 8e-6 treshold: 0.4 batch: 8 epochs: 5
test_sets/pt_test_modified.tsv
Namespace(batch=8, checkpoint='../multilabel/downsampled', epochs=5, full=True, lang='downsampled_pt', learning=8e-06, model='xlm-roberta-large', multilingual=False, saved='saved_models/all_multilingual', test_set=['test_sets/pt_test_modified.tsv'], train_set=['downsampled/full_labels/all_downsampled_full.tsv.gz'], treshold=0.4)
Downloading and preparing dataset csv/default to /users/annieske/.cache/huggingface/datasets/csv/default-e6af68532f4fec99/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...
Dataset csv downloaded and prepared to /users/annieske/.cache/huggingface/datasets/csv/default-e6af68532f4fec99/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.
{'dev': Dataset({
    features: ['text', 'label'],
    num_rows: 1757
}),
 'test': Dataset({
    features: ['text', 'label'],
    num_rows: 332
}),
 'train': Dataset({
    features: ['text', 'label'],
    num_rows: 7027
})}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.123, 'learning_rate': 6.4e-06, 'epoch': 1.0}
{'eval_loss': 0.07373969256877899, 'eval_f1': 0.7271333673990801, 'eval_roc_auc': 0.8377036282210126, 'eval_accuracy': 0.6061468412066021, 'eval_runtime': 53.7011, 'eval_samples_per_second': 32.718, 'eval_steps_per_second': 1.024, 'epoch': 1.0}
{'loss': 0.0641, 'learning_rate': 4.8e-06, 'epoch': 2.0}
{'eval_loss': 0.0630236268043518, 'eval_f1': 0.7603543212832176, 'eval_roc_auc': 0.8762405058840604, 'eval_accuracy': 0.6323278315310188, 'eval_runtime': 53.6824, 'eval_samples_per_second': 32.73, 'eval_steps_per_second': 1.025, 'epoch': 2.0}
{'loss': 0.0487, 'learning_rate': 3.2e-06, 'epoch': 3.0}
{'eval_loss': 0.06282785534858704, 'eval_f1': 0.7608111409723919, 'eval_roc_auc': 0.8694315896264077, 'eval_accuracy': 0.6420034149117815, 'eval_runtime': 53.6722, 'eval_samples_per_second': 32.736, 'eval_steps_per_second': 1.025, 'epoch': 3.0}
{'loss': 0.0386, 'learning_rate': 1.6e-06, 'epoch': 4.0}
{'eval_loss': 0.062436845153570175, 'eval_f1': 0.7634795111430626, 'eval_roc_auc': 0.877557564205391, 'eval_accuracy': 0.6454183266932271, 'eval_runtime': 53.6679, 'eval_samples_per_second': 32.738, 'eval_steps_per_second': 1.025, 'epoch': 4.0}
{'loss': 0.0313, 'learning_rate': 0.0, 'epoch': 5.0}
{'eval_loss': 0.06509667634963989, 'eval_f1': 0.7676428228516562, 'eval_roc_auc': 0.8791654704017976, 'eval_accuracy': 0.6482640865110985, 'eval_runtime': 55.1594, 'eval_samples_per_second': 31.853, 'eval_steps_per_second': 0.997, 'epoch': 5.0}
{'train_runtime': 3969.2624, 'train_samples_per_second': 8.852, 'train_steps_per_second': 1.107, 'train_loss': 0.061124339011477666, 'epoch': 5.0}
F1: 0.5024533856722276
              precision    recall  f1-score   support

          HI       0.81      0.41      0.54        32
          ID       0.62      0.29      0.40        17
          IN       0.43      0.56      0.49        55
          IP       0.77      0.76      0.77       113
          LY       0.00      0.00      0.00         4
          NA       0.77      0.91      0.84       113
          OP       0.50      0.60      0.55        30
          SP       0.00      0.00      0.00         0
          av       0.00      0.00      0.00         6
          ds       0.00      0.00      0.00       106
         dtp       0.00      0.00      0.00        19
          ed       0.00      0.00      0.00         4
          en       0.00      0.00      0.00         3
          fi       0.00      0.00      0.00         1
          it       0.00      0.00      0.00         0
          lt       0.00      0.00      0.00         8
          nb       0.00      0.00      0.00        19
          ne       0.00      0.00      0.00        75
          ob       0.00      0.00      0.00         5
          ra       0.00      0.00      0.00         2
          re       0.00      0.00      0.00         6
          rs       0.00      0.00      0.00         9
          rv       0.00      0.00      0.00         9
          sr       0.00      0.00      0.00         6

   micro avg       0.68      0.40      0.50       642
   macro avg       0.16      0.15      0.15       642
weighted avg       0.39      0.40      0.39       642
 samples avg       0.71      0.41      0.50       642

END: to 23.6.2022 12.01.41 +0300
